# 从0手写大语言模型-上

## 1 目标

https://www.bilibili.com/video/BV1CC411h7Ak?spm_id_from=333.788.videopod.sections&vd_source=5499ea66d45c9e767856755c37a77a97


使用 pytorch-lighting 训练 Decoder-Only 的模型

![alt text](../../../z_using_files/img/article/decoder.png)

### 1.1 训练过程
1. **数据预处理**：将原始文本数据转换为模型可以理解的数字表示。
2. **模型初始化**：初始化模型的参数，包括词嵌入、位置编码等。
3. **前向传播**：将输入数据通过模型，计算输出。
4. **损失计算**：根据模型的输出和真实标签计算损失。
5. **反向传播**：通过梯度下降法更新模型参数。
6. **迭代训练**：重复上述步骤，直到模型收敛。

## 2. 本章节涉及

### 2.1 Tokenization

Tokenization是将文本分割成单词或子词（subword）的过程。每个单词或子词都会被映射到一个唯一的整数ID，从而将文本转换为数字序列。

```python
from transformers import AutoTokenizer

# 加载预训练的tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# 将文本转换为token
text = "Hello, how are you?"
tokens = tokenizer.tokenize(text)
print(tokens)  # 输出: ['hello', ',', 'how', 'are', 'you', '?']

# 将token转换为ID
token_ids = tokenizer.convert_tokens_to_ids(tokens)
print(token_ids)  # 输出: [7592, 1010, 2129, 2024, 2017, 1029]
```
