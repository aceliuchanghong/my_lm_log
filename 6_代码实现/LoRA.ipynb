{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA\n",
    "## Low-Rank Adaptation\n",
    "- LoRA冻结了预训练模型的权重\n",
    "- 将可训练的秩分解矩阵注入到每个Transformer层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LoRA 线性层通过对线性层的预训练权重矩阵 $W_0 \\in \\mathbb{R}^{d \\times k}$ 添加低秩分解来进行扩展。\n",
    "\n",
    "$W_0 + \\Delta W = W_0 + BA$\n",
    "\n",
    "其中 $B \\in \\mathbb{R}^{d \\times r}$，$A \\in \\mathbb{R}^{r \\times k}$，且秩 $r \\ll \\min(d, k)$。\n",
    "\n",
    "所有参数除 $A$ 和 $B$ 外都被冻结。\n",
    "\n",
    "$\\Delta W$ 在训练开始时初始化为零。\n",
    "\n",
    "将 $x \\Delta W^T$ 乘以 $\\frac{\\alpha}{r}$，其中 $\\alpha$ 是一个超参数。一旦 $\\alpha$ 被设定，就保持不变，仅调整 $r$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool,\n",
    "                 r: int, alpha: int = None):\n",
    "        \"\"\"\n",
    "        :param in_features: 输入特征的数量\n",
    "        :param out_features: 输出特征的数量\n",
    "        :param bias: 标志位，指示是否有偏置参数\n",
    "        :param r: 分解的秩 $r$\n",
    "        :param alpha: 缩放因子 $\\alpha$\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 将 $\\frac{\\alpha}{r} = 1$，即设置缩放因子 $\\alpha = r$。\n",
    "        if alpha is None:\n",
    "            alpha = r\n",
    "        # 预训练参数W0\n",
    "        self.weight = nn.Parameter(torch.empty((out_features, in_features)))\n",
    "        # 冻结参数\n",
    "        self.weight.requires_grad = False\n",
    "        if bias:\n",
    "            # Bias $b_0$ (冻结)\n",
    "            self.bias = nn.Parameter(torch.empty(out_features))\n",
    "            self.bias.requires_grad = False\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        self.scaling = alpha / r\n",
    "        self.lora_a = nn.Parameter(torch.empty((r, in_features)))\n",
    "        self.lora_b = nn.Parameter(torch.empty((out_features, r)))\n",
    "        with torch.no_grad():\n",
    "            # 1. 初始化 $A$，类似于普通线性层中的权重矩阵。 a=5 ** 0.5 用于在初始化时控制方差，使模型的梯度更稳定。\n",
    "            # 2. 初始化 $B$ 为 $0$，以确保 $\\Delta W = BA$ 在初始化时为 $0$。\n",
    "            nn.init.kaiming_uniform_(self.lora_a, a=5 ** 0.5)\n",
    "            nn.init.zeros_(self.lora_b)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 计算 $x W_0^T + b_0$\n",
    "        result = nn.functional.linear(x, self.weight, bias=self.bias)\n",
    "        result += (x @ self.lora_a.T @ self.lora_b.T) * self.scaling\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Embedding Layer\n",
    "\n",
    "与 LoRA 线性层类似，此方法在预训练嵌入权重矩阵（$W_0 \\in \\mathbb{R}^{d \\times k}$）中添加了一个低秩分解。\n",
    "\n",
    "$W_0 + \\Delta W = W_0 + BA$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int,\n",
    "                 r: int, alpha: int = None):\n",
    "        super().__init__()\n",
    "\n",
    "        if alpha is None:\n",
    "            alpha = r\n",
    "\n",
    "        self.weight = nn.Parameter(torch.empty((num_embeddings, embedding_dim)))\n",
    "        self.weight.requires_grad = False\n",
    "\n",
    "        self.scaling = alpha / r\n",
    "        self.lora_a = nn.Parameter(torch.empty((r, num_embeddings)))\n",
    "        self.lora_b = nn.Parameter(torch.empty((embedding_dim, r)))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            nn.init.normal_(self.lora_a)\n",
    "            nn.init.zeros_(self.lora_b)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        result = nn.functional.embedding(x, self.weight)\n",
    "        result += (nn.functional.embedding(x, self.lora_a.T) @ self.lora_b.T) * self.scaling\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
