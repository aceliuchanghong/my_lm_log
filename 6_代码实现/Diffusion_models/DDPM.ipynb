{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPM\n",
    "## Denoising Diffusion Probabilistic Models\n",
    "简单来说，我们从数据中获得一张图像，并逐步添加噪声。然后训练一个模型在每个步骤中预测这些噪声，并利用该模型来生成图像。\n",
    "\n",
    "## 正向过程\n",
    "\n",
    "正向过程在 $T$ 个时间步中为数据 $x_0 \\sim q(x_0)$ 添加噪声。\n",
    "\n",
    "\\[\n",
    "q(x_t | x_{t-1}) = \\mathcal{N}\\big(x_t; \\sqrt{1-  \\beta_t} x_{t-1}, \\beta_t \\mathbf{I}\\big) \\\\\n",
    "\\]\n",
    "\\[\n",
    "q(x_{1:T} | x_0) = \\prod_{t = 1}^{T} q(x_t | x_{t-1})\n",
    "\\]\n",
    "\n",
    "其中 $\\beta_1, \\dots, \\beta_T$ 是方差调度。\n",
    "\n",
    "我们可以在任意时间步 $t$ 采样 $x_t$，公式如下：\n",
    "\n",
    "\\[\n",
    "q(x_t|x_0) = \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n",
    "\\]\n",
    "\n",
    "其中 $\\alpha_t = 1 - \\beta_t$，$\\bar\\alpha_t = \\prod_{s=1}^t \\alpha_s$。\n",
    "\n",
    "\n",
    "## 逆过程\n",
    "\n",
    "逆过程从 $p(x_T) = \\mathcal{N}(x_T; \\mathbf{0}, \\mathbf{I})$ 开始，在 $T$ 个时间步中去除噪声。\n",
    "\n",
    "\\[\n",
    "\\textcolor{lightgreen}{p_\\theta}(x_{t-1} | x_t) = \\mathcal{N}\\big(x_{t-1}; \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t), \\textcolor{lightgreen}{\\Sigma_\\theta}(x_t, t)\\big)\n",
    "\\]\n",
    "\\[\n",
    "\\textcolor{lightgreen}{p_\\theta}(x_{0:T}) = \\textcolor{lightgreen}{p_\\theta}(x_T) \\prod_{t = 1}^{T} \\textcolor{lightgreen}{p_\\theta}(x_{t-1} | x_t)\n",
    "\\]\n",
    "\\[\n",
    "\\textcolor{lightgreen}{p_\\theta}(x_0) = \\int \\textcolor{lightgreen}{p_\\theta}(x_{0:T}) \\, dx_{1:T}\n",
    "\\]\n",
    "\n",
    "$\\textcolor{lightgreen}{\\theta}$ 是我们训练的参数。\n",
    "\n",
    "## 损失函数\n",
    "\n",
    "我们通过优化负对数似然上的ELBO（源自詹森不等式）。\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[-\\log \\textcolor{lightgreen}{p_\\theta}(x_0)]\n",
    " \\le \\mathbb{E}_q \\left[ -\\log \\frac{\\textcolor{lightgreen}{p_\\theta}(x_{0:T})}{q(x_{1:T}|x_0)} \\right] = L\n",
    "\\]\n",
    "\n",
    "损失可以重写如下：\n",
    "\n",
    "\\[\n",
    "L = \\mathbb{E}_q \\left[ -\\log \\frac{\\textcolor{lightgreen}{p_\\theta}(x_{0:T})}{q(x_{1:T}|x_0)} \\right] \n",
    "= \\mathbb{E}_q \\left[ -\\log p(x_T) - \\sum_{t=1}^T \\log \\frac{\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1})} \\right] \n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= \\mathbb{E}_q \\left[ -\\log \\frac{p(x_T)}{q(x_T|x_0)} - \\sum_{t=2}^T \\log \\frac{\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)}{q(x_{t-1}|x_t,x_0)} -\\log \\textcolor{lightgreen}{p_\\theta}(x_0|x_1) \\right] \n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= \\mathbb{E}_q \\left[ D_{KL}(q(x_T|x_0) \\Vert p(x_T)) + \\sum_{t=2}^T D_{KL}(q(x_{t-1}|x_t,x_0) \\Vert \\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)) -\\log \\textcolor{lightgreen}{p_\\theta}(x_0|x_1) \\right]\n",
    "\\]\n",
    "\n",
    "由于我们保持 $\\beta_1, \\dots, \\beta_T$ 为常数，因此 $D_{KL}(q(x_T|x_0) \\Vert p(x_T))$ 是一个常数。\n",
    "\n",
    "### Computing $L_{t-1} = D_{KL}(q(x_{t-1}|x_t,x_0) \\Vert \\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t))$\n",
    "\n",
    "The forward process posterior conditioned by $x_0$ is,\n",
    "\n",
    "\\begin{align}\n",
    "q(x_{t-1}|x_t, x_0) &= \\mathcal{N} \\Big(x_{t-1}; \\tilde\\mu_t(x_t, x_0), \\tilde\\beta_t \\mathbf{I} \\Big) \\\\\n",
    "\\tilde\\mu_t(x_t, x_0) &= \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}x_0\n",
    "                         + \\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}x_t \\\\\n",
    "\\tilde\\beta_t &= \\frac{1 - \\bar\\alpha_{t-1}}{1 - \\bar\\alpha_t} \\beta_t\n",
    "\\end{align}\n",
    "\n",
    "The paper sets $\\textcolor{lightgreen}{\\Sigma_\\theta}(x_t, t) = \\sigma_t^2 \\mathbf{I}$ where $\\sigma_t^2$ is set to constants\n",
    "$\\beta_t$ or $\\tilde\\beta_t$.\n",
    "\n",
    "Then,\n",
    "$$\\textcolor{lightgreen}{p_\\theta}(x_{t-1} | x_t) = \\mathcal{N}\\big(x_{t-1}; \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t), \\sigma_t^2 \\mathbf{I} \\big)$$\n",
    "\n",
    "For given noise $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ using $q(x_t|x_0)$\n",
    "\n",
    "\\begin{align}\n",
    "x_t(x_0, \\epsilon) &= \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon \\\\\n",
    "x_0 &= \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\Big(x_t(x_0, \\epsilon) -  \\sqrt{1-\\bar\\alpha_t}\\epsilon\\Big)\n",
    "\\end{align}\n",
    "\n",
    "This gives,\n",
    "\n",
    "\\begin{align}\n",
    "L_{t-1}\n",
    " &= D_{KL}(q(x_{t-1}|x_t,x_0) \\Vert \\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)) \\\\\n",
    " &= \\mathbb{E}_q \\Bigg[ \\frac{1}{2\\sigma_t^2}\n",
    " \\Big \\Vert \\tilde\\mu(x_t, x_0) - \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t) \\Big \\Vert^2 \\Bigg] \\\\\n",
    " &= \\mathbb{E}_{x_0, \\epsilon} \\Bigg[ \\frac{1}{2\\sigma_t^2}\n",
    "  \\bigg\\Vert \\frac{1}{\\sqrt{\\alpha_t}} \\Big(\n",
    "  x_t(x_0, \\epsilon) - \\frac{\\beta_t}{\\sqrt{1 - \\bar\\alpha_t}} \\epsilon\n",
    "  \\Big) - \\textcolor{lightgreen}{\\mu_\\theta}(x_t(x_0, \\epsilon), t) \\bigg\\Vert^2 \\Bigg] \\\\\n",
    "\\end{align}\n",
    "\n",
    "Re-parameterizing with a model to predict noise\n",
    "\n",
    "\\begin{align}\n",
    "\\textcolor{lightgreen}{\\mu_\\theta}(x_t, t) &= \\tilde\\mu \\bigg(x_t,\n",
    "  \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\Big(x_t -\n",
    "   \\sqrt{1-\\bar\\alpha_t}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big) \\bigg) \\\\\n",
    "  &= \\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n",
    "  \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)\n",
    "\\end{align}\n",
    "\n",
    "where $\\epsilon_\\theta$ is a learned function that predicts $\\epsilon$ given $(x_t, t)$.\n",
    "\n",
    "This gives,\n",
    "\n",
    "\\begin{align}\n",
    "L_{t-1}\n",
    "&= \\mathbb{E}_{x_0, \\epsilon} \\Bigg[ \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t (1 - \\bar\\alpha_t)}\n",
    "  \\Big\\Vert\n",
    "  \\epsilon - \\textcolor{lightgreen}{\\epsilon_\\theta}(\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon, t)\n",
    "  \\Big\\Vert^2 \\Bigg]\n",
    "\\end{align}\n",
    "\n",
    "That is, we are training to predict the noise.\n",
    "\n",
    "### Simplified loss\n",
    "\n",
    "$$L_{\\text{simple}}(\\theta) = \\mathbb{E}_{t,x_0, \\epsilon} \\Bigg[ \\bigg\\Vert\n",
    "\\epsilon - \\textcolor{lightgreen}{\\epsilon_\\theta}(\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon, t)\n",
    "\\bigg\\Vert^2 \\Bigg]$$\n",
    "\n",
    "This minimizes $-\\log \\textcolor{lightgreen}{p_\\theta}(x_0|x_1)$ when $t=1$ and $L_{t-1}$ for $t\\gt1$ discarding the\n",
    "weighting in $L_{t-1}$. Discarding the weights $\\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t (1 - \\bar\\alpha_t)}$\n",
    "increase the weight given to higher $t$ (which have higher noise levels), therefore increasing the sample quality.\n",
    "\n",
    "This file implements the loss calculation and a basic sampling method that we use to generate images during\n",
    "training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "from labml_nn.diffusion.ddpm.utils import gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:44: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:62: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:94: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:44: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:62: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:94: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\lawrence\\AppData\\Local\\Temp\\ipykernel_15124\\2607690260.py:7: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  \"\"\"\n",
      "C:\\Users\\lawrence\\AppData\\Local\\Temp\\ipykernel_15124\\2607690260.py:28: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  \"\"\"\n",
      "C:\\Users\\lawrence\\AppData\\Local\\Temp\\ipykernel_15124\\2607690260.py:44: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  \"\"\"\n",
      "C:\\Users\\lawrence\\AppData\\Local\\Temp\\ipykernel_15124\\2607690260.py:62: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  \"\"\"\n",
      "C:\\Users\\lawrence\\AppData\\Local\\Temp\\ipykernel_15124\\2607690260.py:94: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DenoiseDiffusion:\n",
    "    \"\"\"\n",
    "    ## Denoise Diffusion\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eps_model: nn.Module, n_steps: int, device: torch.device):\n",
    "        \"\"\"\n",
    "        * `eps_model` is $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$ model\n",
    "        * `n_steps` is $t$\n",
    "        * `device` is the device to place constants on\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps_model = eps_model\n",
    "\n",
    "        # Create $\\beta_1, \\dots, \\beta_T$ linearly increasing variance schedule\n",
    "        self.beta = torch.linspace(0.0001, 0.02, n_steps).to(device)\n",
    "\n",
    "        # $\\alpha_t = 1 - \\beta_t$\n",
    "        self.alpha = 1. - self.beta\n",
    "        # $\\bar\\alpha_t = \\prod_{s=1}^t \\alpha_s$\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "        # $T$\n",
    "        self.n_steps = n_steps\n",
    "        # $\\sigma^2 = \\beta$\n",
    "        self.sigma2 = self.beta\n",
    "\n",
    "    def q_xt_x0(self, x0: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        #### Get $q(x_t|x_0)$ distribution\n",
    "\n",
    "        \\begin{align}\n",
    "        q(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n",
    "        \\end{align}\n",
    "        \"\"\"\n",
    "\n",
    "        # [gather](utils.html) $\\alpha_t$ and compute $\\sqrt{\\bar\\alpha_t} x_0$\n",
    "        mean = gather(self.alpha_bar, t) ** 0.5 * x0\n",
    "        # $(1-\\bar\\alpha_t) \\mathbf{I}$\n",
    "        var = 1 - gather(self.alpha_bar, t)\n",
    "        #\n",
    "        return mean, var\n",
    "\n",
    "    def q_sample(self, x0: torch.Tensor, t: torch.Tensor, eps: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        #### Sample from $q(x_t|x_0)$\n",
    "\n",
    "        \\begin{align}\n",
    "        q(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n",
    "        \\end{align}\n",
    "        \"\"\"\n",
    "\n",
    "        # $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n",
    "        if eps is None:\n",
    "            eps = torch.randn_like(x0)\n",
    "\n",
    "        # get $q(x_t|x_0)$\n",
    "        mean, var = self.q_xt_x0(x0, t)\n",
    "        # Sample from $q(x_t|x_0)$\n",
    "        return mean + (var ** 0.5) * eps\n",
    "\n",
    "    def p_sample(self, xt: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        #### Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n",
    "\n",
    "        \\begin{align}\n",
    "        \\textcolor{lightgreen}{p_\\theta}(x_{t-1} | x_t) &= \\mathcal{N}\\big(x_{t-1};\n",
    "        \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t), \\sigma_t^2 \\mathbf{I} \\big) \\\\\n",
    "        \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t)\n",
    "          &= \\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n",
    "            \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)\n",
    "        \\end{align}\n",
    "        \"\"\"\n",
    "\n",
    "        # $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$\n",
    "        eps_theta = self.eps_model(xt, t)\n",
    "        # [gather](utils.html) $\\bar\\alpha_t$\n",
    "        alpha_bar = gather(self.alpha_bar, t)\n",
    "        # $\\alpha_t$\n",
    "        alpha = gather(self.alpha, t)\n",
    "        # $\\frac{\\beta}{\\sqrt{1-\\bar\\alpha_t}}$\n",
    "        eps_coef = (1 - alpha) / (1 - alpha_bar) ** .5\n",
    "        # $$\\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n",
    "        #      \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)$$\n",
    "        mean = 1 / (alpha ** 0.5) * (xt - eps_coef * eps_theta)\n",
    "        # $\\sigma^2$\n",
    "        var = gather(self.sigma2, t)\n",
    "\n",
    "        # $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n",
    "        eps = torch.randn(xt.shape, device=xt.device)\n",
    "        # Sample\n",
    "        return mean + (var ** .5) * eps\n",
    "\n",
    "    def loss(self, x0: torch.Tensor, noise: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        #### Simplified Loss\n",
    "\n",
    "        $$L_{\\text{simple}}(\\theta) = \\mathbb{E}_{t,x_0, \\epsilon} \\Bigg[ \\bigg\\Vert\n",
    "        \\epsilon - \\textcolor{lightgreen}{\\epsilon_\\theta}(\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon, t)\n",
    "        \\bigg\\Vert^2 \\Bigg]$$\n",
    "        \"\"\"\n",
    "        # Get batch size\n",
    "        batch_size = x0.shape[0]\n",
    "        # Get random $t$ for each sample in the batch\n",
    "        t = torch.randint(0, self.n_steps, (batch_size,), device=x0.device, dtype=torch.long)\n",
    "\n",
    "        # $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "\n",
    "        # Sample $x_t$ for $q(x_t|x_0)$\n",
    "        xt = self.q_sample(x0, t, eps=noise)\n",
    "        # Get $\\textcolor{lightgreen}{\\epsilon_\\theta}(\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon, t)$\n",
    "        eps_theta = self.eps_model(xt, t)\n",
    "\n",
    "        # MSE loss\n",
    "        return F.mse_loss(noise, eps_theta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
