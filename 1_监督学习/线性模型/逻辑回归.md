逻辑回归是一种常见的监督学习算法，主要用于二分类问题。尽管其名字中带有“回归”，但它本质上是一种分类算法。逻辑回归的核心思想是通过一个线性模型对输入数据进行加权求和后，使用Sigmoid函数（也称为逻辑函数）将输出值映射到[0, 1]之间，从而预测类别概率。

### 逻辑回归模型：
模型假设：
$$
h_{\theta}(x) = \frac{1}{1 + e^{-\theta^T x}}
$$
其中，$h_{\theta}(x)$是预测概率，$\theta$是模型参数，$x$是输入特征向量，$e$是自然对数的底数。

### 损失函数：
逻辑回归使用对数损失（Log Loss）作为优化目标：
$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_{\theta}(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_{\theta}(x^{(i)})) \right]
$$
其中，$m$是样本数，$y^{(i)}$是真实标签，$h_{\theta}(x^{(i)})$是模型预测的概率。

### 代码实现（以Python为例，使用scikit-learn库）：

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 加载数据集
data = load_iris()
X = data.data
y = data.target

# 只选择二分类数据（例如类别0和类别1）
X = X[y != 2]
y = y[y != 2]

# 拆分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 初始化并训练逻辑回归模型
model = LogisticRegression()
model.fit(X_train, y_train)

# 预测测试集
y_pred = model.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print(f'模型准确率: {accuracy:.4f}')
```

### 总结：
逻辑回归通过线性模型与Sigmoid函数结合，解决了二分类问题，并且使用对数损失函数进行优化。它是监督学习中非常基础且实用的一种算法。