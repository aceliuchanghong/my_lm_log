---
theme: gaia
command: moffee live .\3_深度学习\泛化\反向传播.md
command: moffee live /mnt/data/llch/my_lm_log/3_深度学习/泛化/反向传播.md
---

## 反向传播 (Backpropagation)

反向传播是深度学习中一种用于训练神经网络的算法，它通过计算损失函数相对于网络中各个参数的梯度，然后通过梯度下降优化网络的权重。

**反向**体现在从网络的输出层到输入层逐层计算梯度的过程

### 核心概念

1. **前向传播 (Forward Propagation)**  
   - 输入数据通过网络层层传递，最终得到预测值。
   - 计算预测值与实际值之间的差异，得到损失值。

2. **计算梯度 (Gradient Calculation)**  
   - 反向传播的核心是计算损失函数对每个参数的梯度，即每个权重对损失函数的影响程度。
   - 梯度的计算采用链式法则，通过从输出层向输入层逐层传递误差来计算。

3. **梯度下降 (Gradient Descent)**  
   - 使用计算得到的梯度更新网络的权重，以减少损失函数的值。
   - 更新规则：$ w := w - \eta \cdot \frac{\partial L}{\partial w} $，其中 $w$ 是权重，$\eta$ 是学习率，$L$ 是损失函数。

### 反向传播算法步骤

1. **前向传播：**
   - 计算每层的输出，直到得到网络的最终预测。
   
2. **计算损失：**
   - 使用损失函数，计算预测值和真实值之间的差异。

3. **反向传播：**
   - 从输出层开始，计算损失函数相对于每层输出的梯度。
   - 然后利用链式法则，计算每一层权重和偏置的梯度。

4. **更新参数：**
   - 使用梯度下降法或其变种（如Adam）来更新权重和偏置。

### 数学推导

假设我们有一个简单的神经网络，损失函数为 $L$，网络的输出为 $\hat{y}$，真实标签为 $y$。我们希望计算损失函数对网络参数（如权重 $w$ 和偏置 $b$）的梯度。

1. **输出层的梯度：**
   - 对输出层的损失函数 $L$ 相对于网络输出 $\hat{y}$ 的偏导数：
     $$ \frac{\partial L}{\partial \hat{y}} $$

2. **隐藏层的梯度：**
   - 使用链式法则计算损失对每一层的梯度：
     $$ \frac{\partial L}{\partial z} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} $$

   - 其中，$z$ 是某一层的加权输入，$\hat{y}$ 是输出。

3. **更新权重：**
   - 使用梯度更新规则更新每一层的权重和偏置：
     $$ w := w - \eta \cdot \frac{\partial L}{\partial w} $$

### 代码实现

下面是一个简单的 Python 示例，使用 PyTorch 实现反向传播过程：

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 假设我们有一个简单的神经网络
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 2)
        self.fc2 = nn.Linear(2, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# 创建模型、损失函数和优化器
model = SimpleNN()
criterion = nn.MSELoss()  # 使用均方误差作为损失函数
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 示例输入和标签
inputs = torch.tensor([[1.0, 2.0]], requires_grad=True)
labels = torch.tensor([[1.0]])

# 前向传播
outputs = model(inputs)
loss = criterion(outputs, labels)

# 反向传播
optimizer.zero_grad()  # 清除梯度
loss.backward()  # 计算梯度

# 更新参数
optimizer.step()  # 更新权重

print(f"Loss: {loss.item()}")
```

### 常见问题

- **梯度消失/爆炸**：在深层网络中，梯度可能会变得非常小（消失）或非常大（爆炸），导致网络无法有效训练。常见的解决方法包括使用合适的激活函数（如ReLU）、初始化方法和归一化技术（如批归一化）。
  
- **学习率选择**：学习率过大可能导致参数更新过快，错过最优解；学习率过小可能导致收敛过慢。使用学习率调度器（如Adam优化器）可以自动调整学习率。
