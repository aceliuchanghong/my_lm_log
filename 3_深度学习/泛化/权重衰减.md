## 权重衰减（Weight Decay）
是机器学习中一种常见的正则化方法，用于防止模型过拟合。它通过对模型的权重施加惩罚，限制模型的复杂度，从而提高其泛化能力。

### 核心概念

权重衰减的主要思想是在损失函数中加入一个惩罚项，该惩罚项与模型权重的大小成正比。这样可以迫使模型在训练时保持较小的权重值，避免过拟合。损失函数通常会有两部分：

1. **原始损失函数**：例如均方误差（MSE）或交叉熵损失（Cross Entropy Loss）。
2. **权重衰减项**：这个项通常是所有权重的L2范数，表示权重的平方和。

损失函数可以表示为：

$$ L_{total} = L_{original} + \lambda \sum w^2 $$

其中，$L_{total}$ 是总损失，$L_{original}$ 是原始损失，$\lambda$ 是权重衰减系数，$w$ 是模型的权重参数。

### 常见问题

1. **为什么要使用权重衰减？**
   权重衰减是一种正则化技术，可以防止模型在训练数据上表现得过于“精准”，从而避免过拟合。它使模型在新数据上有更好的泛化能力。

2. **权重衰减与L2正则化有什么关系？**
   实际上，权重衰减与L2正则化是等价的。它们都通过对权重施加惩罚来限制模型的复杂度，只是表达方式不同。

3. **如何选择权重衰减系数 $\lambda$？**
   $\lambda$ 的选择需要通过交叉验证来调整。$\lambda$ 过大可能导致欠拟合，过小则无法有效抑制过拟合。

### 代码实现

在Python中使用PyTorch，可以通过在优化器中设置`weight_decay`参数来实现权重衰减。例如：

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 定义一个简单的线性模型
model = nn.Linear(10, 1)

# 定义损失函数
criterion = nn.MSELoss()

# 定义优化器，并设置权重衰减
optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)

# 输入数据
inputs = torch.randn(5, 10)
targets = torch.randn(5, 1)

# 前向传播
outputs = model(inputs)
loss = criterion(outputs, targets)

# 反向传播和优化
loss.backward()
optimizer.step()
```

在上面的代码中，`weight_decay=0.001` 即为权重衰减系数 $\lambda$。

### 数学推导

从数学上看，L2正则化（即权重衰减）会对每一个权重施加一个与其大小成正比的惩罚。假设原始损失函数为 $L(w)$，那么添加权重衰减后的总损失函数为：

$$ L_{total}(w) = L(w) + \lambda \sum_{i} w_i^2 $$

在梯度下降法中，我们需要计算这个总损失函数关于权重 $w$ 的梯度：

$$ \frac{\partial L_{total}}{\partial w_i} = \frac{\partial L(w)}{\partial w_i} + 2\lambda w_i $$

因此，梯度更新的规则变为：

$$ w_i \leftarrow w_i - \eta \left( \frac{\partial L(w)}{\partial w_i} + 2\lambda w_i \right) $$

这意味着权重不仅会根据损失函数的梯度更新，还会被 $\lambda$ 系数所衰减。

### 总结

权重衰减通过对模型的权重施加L2正则化，防止模型在训练数据上过拟合，提升模型在新数据上的泛化能力。在实践中，调节适当的权重衰减系数非常重要，可以通过交叉验证选择合适的 $\lambda$。