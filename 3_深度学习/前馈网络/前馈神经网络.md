## 前馈神经网络（Feedforward Neural Network, FNN）
是一种最简单的人工神经网络结构，其特点是信息从输入层经过隐藏层到输出层，始终向前流动，不存在反馈或循环。

## 1. 核心概念
- **输入层**：输入特征的向量。
- **隐藏层**：应用激活函数，非线性地映射输入数据。
- **输出层**：产生最终的预测结果。
- **权重和偏置**：网络中每条连接的权重和每个神经元的偏置是通过训练来学习的。
- **激活函数**：常见的激活函数有Sigmoid、ReLU、Tanh等，用来引入非线性。

## 2. 常见问题与解答
- **Q**: 为什么需要隐藏层？
  - **A**: 隐藏层引入了非线性，使网络能够逼近更复杂的函数，从而提高模型的表达能力。
  
- **Q**: 为什么需要激活函数？
  - **A**: 如果没有激活函数，网络的输出只是线性组合，无法表达复杂的非线性关系。
  
- **Q**: 前馈神经网络如何训练？
  - **A**: 通过反向传播算法更新权重和偏置，最小化损失函数。

## 3. 代码实现
下面是一个使用PyTorch实现简单前馈网络的示例：

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 定义前馈神经网络类
class FeedforwardNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(FeedforwardNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

# 参数
input_size = 10   # 输入层大小
hidden_size = 5   # 隐藏层大小
output_size = 1   # 输出层大小
learning_rate = 0.001

# 实例化网络
model = FeedforwardNN(input_size, hidden_size, output_size)

# 损失函数和优化器
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# 假设输入和目标输出
inputs = torch.randn(1, input_size)
targets = torch.randn(1, output_size)

# 前向传播和反向传播
outputs = model(inputs)
loss = criterion(outputs, targets)
loss.backward()
optimizer.step()

print(f'Loss: {loss.item()}')
```

## 4. 数学公式理解
在前馈神经网络中，输入 $X$ 经过权重矩阵 $W$ 和偏置向量 $b$ 的线性变换，然后通过激活函数 $f$：

$$ Z = f(WX + b) $$

其中，$W$ 是权重矩阵，$b$ 是偏置向量，$f$ 是激活函数。通过反向传播计算梯度，逐步调整 $W$ 和 $b$ 来最小化损失函数。