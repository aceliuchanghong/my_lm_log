### 最小二乘法

最小二乘法（Least Squares Method）是统计学中广泛使用的一种拟合方法，通常用于线性回归模型中。它的目的是通过最小化预测值与实际值之间的误差的平方和，找到最佳拟合的回归参数。

#### 核心概念

* **线性回归模型**：
  $$
  y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_nx_n + \epsilon
  $$
  其中：
  - $ y $ 是响应变量。
  - $ x_i $ 是预测变量。
  - $ \beta_i $ 是回归系数。
  - $ \epsilon $ 是误差项。

* **目标**：通过最小化预测值 $ \hat{y} $ 与实际值 $ y $ 之间的误差平方和（Sum of Squared Errors），即：
  $$
  \text{SSE} = \sum_{i=1}^n (y_i - \hat{y}_i)^2
  $$
  来找到最优的回归系数 $ \beta_i $。

#### 常见问题及答案

1. **什么是残差？**
    - 残差是实际值与预测值之间的差异，即 $ e_i = y_i - \hat{y}_i $。

2. **为什么最小二乘法使用平方和而不是绝对值和？**
    - 使用平方和可以方便地进行微分运算，从而找到闭形式的解。同时，这样使得对较大误差更加敏感，便于消除异常点。

3. **如何计算回归系数？**
    - 通过求解关于回归系数的平方和问题的导数，得到方程组，从而计算回归系数。

#### 代码实现

下面是使用Python和`NumPy`实现的简单线性回归的最小二乘法：

```python
import numpy as np

# 生成模拟数据
np.random.seed(0)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# 添加截距项 x0 = 1
X_b = np.c_[np.ones((100, 1)), X]

# 最小二乘法求解回归系数
theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y

print("回归系数:", theta_best)
```

#### 数学公式的推导

最小二乘法的关键在于通过最小化误差平方和来确定回归系数。这可以通过对误差平方和函数进行微分并求解得出。

误差平方和函数：
$$
\text{SSE} = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1x_{i1} + \cdots + \beta_nx_{in}))^2
$$

对每个回归系数求导并令其等于零：
$$
\frac{\partial \text{SSE}}{\partial \beta_j} = -2 \sum_{i=1}^n (y_i - \hat{y_i}) x_{ij} = 0
$$

将方程组转化为矩阵形式：
$$
\mathbf{X}^T \mathbf{X} \beta = \mathbf{X}^T \mathbf{y}
$$

求解得到回归系数：
$$
\beta = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$
