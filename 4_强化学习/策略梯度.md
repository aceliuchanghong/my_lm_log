## 策略梯度（Policy Gradient）
是一类通过直接对策略进行优化的方法，它通过提升行动选择的概率来增加累积奖励。策略梯度方法在策略优化中尤为重要，因为它能够处理离散和连续的动作空间。

### 1. 核心概念

策略梯度的核心思想是：**通过最大化累积奖励来优化策略**。而最常见的策略梯度算法是REINFORCE（“蒙特卡洛策略梯度”）和基于近似策略优化的算法，如PPO（Proximal Policy Optimization）。

#### 策略表示：
策略$ \pi_{\theta}(a|s) $表示在状态$ s $下选择动作$ a $的概率，这个策略是一个由参数$ \theta $控制的概率分布函数。策略梯度方法的目标是通过梯度上升（或者下降）来优化这个策略的参数$ \theta $。

#### 优化目标：
我们想要最大化的是**预期累积奖励**：
$$ J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ R(\tau) \right] $$
其中，$ \tau $是一次完整的状态-动作轨迹，$ R(\tau) $是这条轨迹的总奖励。

#### 策略梯度定理：
策略梯度定理给出了如何计算梯度来优化参数：
$$ \nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \nabla_{\theta} \log \pi_{\theta}(a|s) \cdot R(\tau) \right] $$
这个公式表明，可以通过对策略的对数概率进行求导，然后乘以奖励值来更新策略参数。

### 2. 常见问题与解答

**问题 1：为什么要使用策略梯度而不是值函数方法？**  
答：值函数方法（如Q-learning）通常要求确定性策略，而策略梯度可以处理**连续动作空间**或**非确定性策略**，且在某些场景下，策略梯度方法的表现会优于基于值函数的方法。

**问题 2：策略梯度方法有哪些常见的缺陷？**  
答：策略梯度方法的一个主要问题是它的方差通常较大，导致收敛速度较慢。通常会使用基线函数（如状态值函数）来减小方差。

**问题 3：REINFORCE 算法如何减小方差？**  
答：REINFORCE通过引入一个**基线**（通常是状态的值函数）来减小策略梯度的方差。这使得更新过程更稳定。

### 3. 代码实现

使用`PyTorch`实现一个简单的策略梯度算法（REINFORCE）：

```python
import torch
import torch.nn as nn
import torch.optim as optim
import gym

# 定义策略网络
class PolicyNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(PolicyNetwork, self).__init__()
        self.fc = nn.Linear(state_size, 128)
        self.output = nn.Linear(128, action_size)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        x = torch.relu(self.fc(x))
        return self.softmax(self.output(x))

# 选择动作
def select_action(policy, state):
    state = torch.from_numpy(state).float().unsqueeze(0)
    probs = policy(state)
    action = torch.multinomial(probs, 1).item()
    return action, torch.log(probs[0, action])

# 更新策略参数
def update_policy(optimizer, rewards, log_probs, gamma=0.99):
    discounted_rewards = []
    R = 0
    for r in rewards[::-1]:
        R = r + gamma * R
        discounted_rewards.insert(0, R)
    discounted_rewards = torch.tensor(discounted_rewards)
    
    # 归一化奖励
    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)
    
    policy_loss = []
    for log_prob, reward in zip(log_probs, discounted_rewards):
        policy_loss.append(-log_prob * reward)
    
    optimizer.zero_grad()
    policy_loss = torch.cat(policy_loss).sum()
    policy_loss.backward()
    optimizer.step()

# 环境初始化
env = gym.make('CartPole-v1')
policy = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)
optimizer = optim.Adam(policy.parameters(), lr=1e-2)

# 训练
for episode in range(1000):
    state = env.reset()
    log_probs = []
    rewards = []
    
    for t in range(1000):
        action, log_prob = select_action(policy, state)
        next_state, reward, done, _ = env.step(action)
        log_probs.append(log_prob)
        rewards.append(reward)
        
        if done:
            break
        
        state = next_state
    
    update_policy(optimizer, rewards, log_probs)
    if episode % 100 == 0:
        print(f'Episode {episode} finished after {t+1} timesteps')

env.close()
```

### 4. 数学推导

在推导策略梯度定理时，首先从期望奖励的目标函数出发：
$ J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ R(\tau) \right] $
利用链式法则对$ \theta $求导可以得到：
$ \nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \nabla_{\theta} \log \pi_{\theta}(\tau) \cdot R(\tau) \right] $
其中，$ \log \pi_{\theta}(\tau) $表示整个轨迹的对数概率。通过计算轨迹上的每一步对数概率之和，可以得到具体的梯度表达式。

这种方法的一个关键问题是奖励的方差较大，因此通常使用**优势函数** $ A(s, a) = Q(s, a) - V(s) $来降低方差。

---
